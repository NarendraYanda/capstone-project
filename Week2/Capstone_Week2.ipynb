{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgn4sGxVmaUk",
        "outputId": "adb36d0b-0aa3-49e2-f1d2-589d3ad1da3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: tabula-py in /usr/local/lib/python3.10/dist-packages (2.9.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.0)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from tabula-py) (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tabula-py) (1.26.4)\n",
            "Requirement already satisfied: distro in /usr/lib/python3/dist-packages (from tabula-py) (1.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2024.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 PDF file(s) in /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF: ['KNOWLABS,INC_08_15_2005-EX-10-INTELLECTUAL PROPERTY AGREEMENT.PDF', 'ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf']\n",
            "\n",
            "Processing file: KNOWLABS,INC_08_15_2005-EX-10-INTELLECTUAL PROPERTY AGREEMENT.PDF\n",
            "Processing Page 1 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/KNOWLABS,INC_08_15_2005-EX-10-INTELLECTUAL PROPERTY AGREEMENT.PDF\n",
            "Processing Page 2 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/KNOWLABS,INC_08_15_2005-EX-10-INTELLECTUAL PROPERTY AGREEMENT.PDF\n",
            "Processing Page 3 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/KNOWLABS,INC_08_15_2005-EX-10-INTELLECTUAL PROPERTY AGREEMENT.PDF\n",
            "\n",
            "Processing file: ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 1 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 2 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 3 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 4 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 5 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 6 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 7 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 8 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 9 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 10 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 11 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 12 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 13 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 14 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 15 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 16 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 17 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 18 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 19 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 20 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 21 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 22 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 23 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 24 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 25 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 26 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 27 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 28 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 29 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 30 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 31 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 32 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 33 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 34 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 35 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 36 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 37 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 38 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 39 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "Processing Page 40 of /content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF/ArmstrongFlooringInc_20190107_8-K_EX-10.2_11471795_EX-10.2_Intellectual Property Agreement.pdf\n",
            "\n",
            "Comparing text representations for 2 document(s).\n",
            "\n",
            "Text Representation Comparison:\n",
            "                   Method  Mean Cosine Similarity\n",
            "0            Bag of Words                0.861411\n",
            "1                  TF-IDF                0.796873\n",
            "2                Word2Vec                0.999990\n",
            "3  Transformer Embeddings                0.893177\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Install Required Libraries\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pytesseract pdfplumber tabula-py\n",
        "\n",
        "# Step 3: Import Libraries\n",
        "import os\n",
        "import re\n",
        "import tabula\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import io\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load Spacy model for Named Entity Recognition\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except:\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Load transformer model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "transformer_model = TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing function for legal documents\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    processed_text = \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
        "    return processed_text\n",
        "\n",
        "# Bag of Words representation\n",
        "def get_bow(texts):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X\n",
        "\n",
        "# TF-IDF representation\n",
        "def get_tfidf(texts):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X\n",
        "\n",
        "# Word2Vec representation\n",
        "def get_word2vec(texts):\n",
        "    sentences = [text.split() for text in texts]\n",
        "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    vectors = []\n",
        "    for sentence in sentences:\n",
        "        word_vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
        "        if word_vectors:\n",
        "            vectors.append(np.mean(word_vectors, axis=0))\n",
        "        else:\n",
        "            vectors.append(np.zeros(model.vector_size))\n",
        "    return np.array(vectors)\n",
        "\n",
        "# Transformer Embeddings representation\n",
        "def get_transformer_embeddings(texts):\n",
        "    inputs = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
        "    outputs = transformer_model(inputs['input_ids'])\n",
        "    embeddings = tf.reduce_mean(outputs.last_hidden_state, axis=1).numpy()\n",
        "    return embeddings\n",
        "\n",
        "# Compare different text representation methods\n",
        "def compare_representations(texts):\n",
        "    data = []\n",
        "\n",
        "    if len(texts) < 2:\n",
        "        print(\"Need at least two documents to compare representations.\")\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    # Bag of Words\n",
        "    bow_matrix = get_bow(texts)\n",
        "    bow_similarity = cosine_similarity(bow_matrix)\n",
        "    data.append({\n",
        "        \"Method\": \"Bag of Words\",\n",
        "        \"Mean Cosine Similarity\": np.mean(bow_similarity[np.triu_indices_from(bow_similarity, k=1)])\n",
        "    })\n",
        "\n",
        "    # TF-IDF\n",
        "    tfidf_matrix = get_tfidf(texts)\n",
        "    tfidf_similarity = cosine_similarity(tfidf_matrix)\n",
        "    data.append({\n",
        "        \"Method\": \"TF-IDF\",\n",
        "        \"Mean Cosine Similarity\": np.mean(tfidf_similarity[np.triu_indices_from(tfidf_similarity, k=1)])\n",
        "    })\n",
        "\n",
        "    # Word2Vec\n",
        "    w2v_vectors = get_word2vec(texts)\n",
        "    w2v_similarity = cosine_similarity(w2v_vectors)\n",
        "    data.append({\n",
        "        \"Method\": \"Word2Vec\",\n",
        "        \"Mean Cosine Similarity\": np.mean(w2v_similarity[np.triu_indices_from(w2v_similarity, k=1)])\n",
        "    })\n",
        "\n",
        "    # Transformer Embeddings\n",
        "    transformer_embeddings = get_transformer_embeddings(texts)\n",
        "    transformer_similarity = cosine_similarity(transformer_embeddings)\n",
        "    data.append({\n",
        "        \"Method\": \"Transformer Embeddings\",\n",
        "        \"Mean Cosine Similarity\": np.mean(transformer_similarity[np.triu_indices_from(transformer_similarity, k=1)])\n",
        "    })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Function for text normalization\n",
        "def text_normalization(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Function to lemmatize tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    pos_tagged = pos_tag(tokens)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tagged]\n",
        "    return lemmatized_words\n",
        "\n",
        "# Function to get WordNet POS tags\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Function for Named Entity Recognition (NER) using Spacy\n",
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(words):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "# Function to extract tables using Tabula-py and capture context around the table\n",
        "def extract_tables_and_context(pdf_path, page_text, page_num, previous_page_text=None, lines_above=3, lines_below=3):\n",
        "    try:\n",
        "        tables = tabula.read_pdf(pdf_path, pages=page_num, multiple_tables=True, lattice=True, stream=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting tables from page {page_num} of {pdf_path}: {e}\")\n",
        "        tables = []\n",
        "\n",
        "    table_list = []\n",
        "    all_lines = page_text.splitlines()\n",
        "\n",
        "    if tables:\n",
        "        for i, table in enumerate(tables):\n",
        "            # Capture context above: If table is near the top of the page, look at the previous page's content\n",
        "            if i == 0 and previous_page_text:\n",
        "                previous_page_lines = previous_page_text.splitlines()\n",
        "                context_above = \"\\n\".join(previous_page_lines[-lines_above:])  # Get lines from the previous page\n",
        "            else:\n",
        "                # Assuming table starts at line `i`, which might not be accurate. Adjust as needed.\n",
        "                context_above = \"\\n\".join(all_lines[max(0, i - lines_above):i])\n",
        "\n",
        "            # Capture context below\n",
        "            context_below = \"\\n\".join(all_lines[i + len(table):i + len(table) + lines_below])\n",
        "\n",
        "            table_list.append({\n",
        "                \"table_number\": i + 1,\n",
        "                \"table_data\": table,\n",
        "                \"context_above\": context_above,\n",
        "                \"context_below\": context_below\n",
        "            })\n",
        "    return table_list\n",
        "\n",
        "# Function to extract PDF content with OCR and tables with context\n",
        "def extract_pdf_content_with_ocr(pdf_path, lines_above=3, lines_below=3):\n",
        "    full_text = \"\"\n",
        "    table_context_data = []\n",
        "    previous_page_text = None\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_num, page in enumerate(pdf.pages, start=1):\n",
        "            print(f\"Processing Page {page_num} of {pdf_path}\")\n",
        "            page_text = page.extract_text()\n",
        "\n",
        "            if page_text:\n",
        "                full_text += page_text + \"\\n\\n\"\n",
        "\n",
        "                # Extract tables and context from this page\n",
        "                tables_with_context = extract_tables_and_context(pdf_path, page_text, page_num, previous_page_text, lines_above, lines_below)\n",
        "                if tables_with_context:\n",
        "                    table_context_data.append({\n",
        "                        \"page_number\": page_num,\n",
        "                        \"tables\": tables_with_context\n",
        "                    })\n",
        "\n",
        "                previous_page_text = page_text  # Store the current page's text for context on the next page\n",
        "            else:\n",
        "                print(f\"No extractable text found on Page {page_num} of {pdf_path}. Using OCR.\")\n",
        "                page_image = page.to_image()\n",
        "                image_bytes = page_image.original\n",
        "                img = Image.open(io.BytesIO(image_bytes))\n",
        "                ocr_text = pytesseract.image_to_string(img)\n",
        "                full_text += ocr_text + \"\\n\\n\"\n",
        "\n",
        "    return full_text, table_context_data\n",
        "\n",
        "# Function to process PDF files and extract tables with context\n",
        "def process_files(pdf_directory):\n",
        "    file_names = [f for f in os.listdir(pdf_directory) if f.lower().endswith('.pdf')]\n",
        "    print(f\"Found {len(file_names)} PDF file(s) in {pdf_directory}: {file_names}\")\n",
        "    all_preprocessed_data = []\n",
        "\n",
        "    for file_name in file_names:\n",
        "        base_name = os.path.splitext(file_name)[0]\n",
        "        pdf_path = os.path.join(pdf_directory, file_name)\n",
        "        print(f\"\\nProcessing file: {file_name}\")\n",
        "\n",
        "        # Extract content from PDF using OCR and Tabula\n",
        "        pdf_content, table_context_data = extract_pdf_content_with_ocr(pdf_path)\n",
        "\n",
        "        # Pre-process the content\n",
        "        normalized_text = text_normalization(pdf_content)\n",
        "        words = word_tokenize(normalized_text)\n",
        "        words = remove_stopwords(words)\n",
        "        lemmatized_words = lemmatize_tokens(words)\n",
        "        named_entities = named_entity_recognition(normalized_text)\n",
        "\n",
        "        # Store the pre-processed data\n",
        "        all_preprocessed_data.append({\n",
        "            \"file_name\": file_name,\n",
        "            \"normalized_text\": normalized_text,\n",
        "            \"lemmatized_words\": list(lemmatized_words),\n",
        "            \"named_entities\": named_entities,\n",
        "            \"table_context_data\": table_context_data  # Include tables with their context\n",
        "        })\n",
        "\n",
        "    return all_preprocessed_data\n",
        "\n",
        "# Compare text representations using the real pre-processed data\n",
        "def run_comparison_with_real_data(preprocessed_data):\n",
        "    if not preprocessed_data:\n",
        "        print(\"No preprocessed data available for comparison.\")\n",
        "        return\n",
        "\n",
        "    normalized_texts = [data['normalized_text'] for data in preprocessed_data]\n",
        "    print(f\"\\nComparing text representations for {len(normalized_texts)} document(s).\")\n",
        "    df_comparison = compare_representations(normalized_texts)\n",
        "    if not df_comparison.empty:\n",
        "        print(\"\\nText Representation Comparison:\")\n",
        "        print(df_comparison)\n",
        "    else:\n",
        "        print(\"Comparison DataFrame is empty.\")\n",
        "\n",
        "# Specify directories\n",
        "pdf_directory = \"/content/drive/MyDrive/AIML/Capstone-Project/data/LimitedData/PDF\"\n",
        "\n",
        "# Process all files\n",
        "all_preprocessed_data = process_files(pdf_directory)\n",
        "\n",
        "# Run comparison using the pre-processed data\n",
        "run_comparison_with_real_data(all_preprocessed_data)\n"
      ]
    }
  ]
}