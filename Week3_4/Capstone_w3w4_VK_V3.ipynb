{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09d9734-09ed-4099-996d-96b6550d3004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tabula\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load Spacy model for Named Entity Recognition\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function for text normalization\n",
    "def text_normalization(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Function to lemmatize tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tagged = pos_tag(tokens)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tagged]\n",
    "    return lemmatized_words\n",
    "\n",
    "# Function to get WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function for Named Entity Recognition (NER) using Spacy\n",
    "def named_entity_recognition(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Function to extract tables using Tabula-py and capture context around the table\n",
    "def extract_tables_and_context(pdf_path, page_text, page_num, previous_page_text=None, lines_above=3, lines_below=3):\n",
    "    tables = tabula.read_pdf(pdf_path, pages=page_num, multiple_tables=True)\n",
    "    table_list = []\n",
    "    all_lines = page_text.splitlines()\n",
    "\n",
    "    if tables:\n",
    "        for i, table in enumerate(tables):\n",
    "            if i == 0 and previous_page_text:\n",
    "                previous_page_lines = previous_page_text.splitlines()\n",
    "                context_above = \"\\n\".join(previous_page_lines[-lines_above:])\n",
    "            else:\n",
    "                context_above = \"\\n\".join(all_lines[max(0, i - lines_above):i])\n",
    "            context_below = \"\\n\".join(all_lines[i + len(table):i + len(table) + lines_below])\n",
    "            table_list.append({\n",
    "                \"table_number\": i + 1,\n",
    "                \"table_data\": table,\n",
    "                \"context_above\": context_above,\n",
    "                \"context_below\": context_below\n",
    "            })\n",
    "    return table_list\n",
    "\n",
    "# Function to process PDF files and extract tables with context\n",
    "def process_files(pdf_directory):\n",
    "    file_names = [f for f in os.listdir(pdf_directory) if f.lower().endswith('.pdf')]\n",
    "    all_preprocessed_data = []\n",
    "\n",
    "    for file_name in file_names:\n",
    "        pdf_path = os.path.join(pdf_directory, file_name)\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages):\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        table_context_data = extract_tables_and_context(pdf_path, page_text, page_num + 1)\n",
    "                        normalized_text = text_normalization(page_text)\n",
    "                        words = word_tokenize(normalized_text)\n",
    "                        words = remove_stopwords(words)\n",
    "                        lemmatized_words = lemmatize_tokens(words)\n",
    "                        named_entities = named_entity_recognition(normalized_text)\n",
    "                        all_preprocessed_data.append({\n",
    "                            \"file_name\": file_name,\n",
    "                            \"normalized_text\": normalized_text,\n",
    "                            \"lemmatized_words\": lemmatized_words,\n",
    "                            \"named_entities\": named_entities,\n",
    "                            \"table_context_data\": table_context_data\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "    return all_preprocessed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e3306-8bf7-40bc-81a6-a921d4866d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to generate document embeddings\n",
    "def get_document_embedding(normalized_text, named_entities, table_context_data):\n",
    "    entity_text = \" \".join([entity[0] for entity in named_entities])\n",
    "    table_text = \" \".join([table[\"context_above\"] + \" \" + table[\"context_below\"] for page in table_context_data for table in page[\"tables\"]])\n",
    "    combined_text = normalized_text + \" \" + entity_text + \" \" + table_text\n",
    "    inputs = tokenizer(combined_text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "    outputs = bert_model(inputs['input_ids'])\n",
    "    embedding = tf.reduce_mean(outputs.last_hidden_state, axis=1).numpy()\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02dc34-e84c-4067-b359-ac8072784668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Function to initialize FAISS and perform clustering\n",
    "def prepare_faiss_and_clusters(all_preprocessed_data, num_clusters):\n",
    "    all_document_embeddings = []\n",
    "    for data in all_preprocessed_data:\n",
    "        embedding = get_document_embedding(\n",
    "            data['normalized_text'],\n",
    "            data['named_entities'],\n",
    "            data['table_context_data']\n",
    "        )\n",
    "        all_document_embeddings.append(embedding)\n",
    "\n",
    "    all_document_embeddings = np.array(all_document_embeddings)\n",
    "    \n",
    "    # Initialize FAISS index and add embeddings\n",
    "    embedding_dimension = all_document_embeddings.shape[1]\n",
    "    faiss_index = faiss.IndexFlatL2(embedding_dimension)\n",
    "    faiss_index.add(all_document_embeddings)\n",
    "\n",
    "    # Perform K-Means clustering\n",
    "    kmeans_model = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    clusters = kmeans_model.fit_predict(all_document_embeddings)\n",
    "\n",
    "    return faiss_index, all_document_embeddings, clusters, kmeans_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8245d6b-1c0d-483c-aca0-6d76fc37c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Function for BERT-based extractive summarization\n",
    "def bert_extractive_summary(normalized_text, named_entities, max_sentences=3):\n",
    "    sentences = sent_tokenize(normalized_text)\n",
    "    sentence_embeddings = get_bert_embeddings(sentences)\n",
    "    document_embedding = get_document_embedding(normalized_text, named_entities, [])\n",
    "    similarity_scores = cosine_similarity(sentence_embeddings, document_embedding.reshape(1, -1)).flatten()\n",
    "    ranked_sentences = [sentences[i] for i in np.argsort(similarity_scores)[::-1]]\n",
    "    return \" \".join(ranked_sentences[:max_sentences])\n",
    "\n",
    "# Function for LLaMA-based abstractive summarization\n",
    "def generate_llama_summary_with_context(normalized_text, named_entities, table_context_data):\n",
    "    entity_text = \" \".join([entity[0] for entity in named_entities])\n",
    "    table_text = \" \".join([table[\"context_above\"] + \" \" + table[\"context_below\"] for page in table_context_data for table in page[\"tables\"]])\n",
    "    combined_text = normalized_text + \"\\n\\nNamed Entities: \" + entity_text + \"\\n\\nTable Context: \" + table_text\n",
    "    \n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\"model\": \"llama3.1\", \"prompt\": f\"Summarize the following text:\\n\\n{combined_text}\\n\\nAbstract Summary:\"}\n",
    "    \n",
    "    response = requests.post(\"http://127.0.0.1:11434/api/generate\", headers=headers, data=json.dumps(data), stream=True)\n",
    "    final_summary = \"\"\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                data = json.loads(line.decode('utf-8'))\n",
    "                final_summary += data.get(\"response\", \"\")\n",
    "                if data.get(\"done\", False):\n",
    "                    break\n",
    "            except json.JSONDecodeError as e:\n",
    "                continue\n",
    "\n",
    "    return final_summary.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b1e7f-60b0-4fac-b805-7797ef1e3117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_summary_mode(all_preprocessed_data, faiss_index, clusters, kmeans_model, mode=\"query\", query=None, top_k=5):\n",
    "    if mode == \"query\" and query:\n",
    "        query_embedding, _ = preprocess_query(query)\n",
    "        \n",
    "        # Predict the cluster for the query\n",
    "        query_cluster = kmeans_model.predict([query_embedding])[0]\n",
    "        print(f\"Query belongs to cluster: {query_cluster}\")\n",
    "        \n",
    "        # Get relevant documents from the predicted cluster\n",
    "        relevant_indices = [i for i, cluster in enumerate(clusters) if cluster == query_cluster]\n",
    "        \n",
    "        # Create cluster-specific FAISS index for fast search\n",
    "        cluster_embeddings = np.array([get_document_embedding(all_preprocessed_data[i]['normalized_text'], all_preprocessed_data[i]['named_entities'], all_preprocessed_data[i]['table_context_data']) for i in relevant_indices])\n",
    "        cluster_faiss_index = initialize_faiss_index(embedding_dimension=768)\n",
    "        add_embeddings_to_faiss(cluster_faiss_index, cluster_embeddings)\n",
    "        \n",
    "        # Perform FAISS search within the cluster to find the top-K similar documents\n",
    "        distances, cluster_specific_indices = search_faiss(cluster_faiss_index, query_embedding, top_k=top_k)\n",
    "\n",
    "        # Retrieve and display the top-K results, ranked by similarity (smallest distance = most similar)\n",
    "        print(\"\\nTop documents ranked by similarity to the query:\")\n",
    "        for rank, i in enumerate(cluster_specific_indices[0]):\n",
    "            original_index = relevant_indices[i]\n",
    "            doc = all_preprocessed_data[original_index]\n",
    "            distance = distances[0][rank]  # Get the corresponding distance for the document\n",
    "            \n",
    "            print(f\"Rank {rank+1}, Document: {doc['file_name']}, Distance: {distance}\")\n",
    "            abstractive_summary = generate_llama_summary_with_context(doc['normalized_text'], doc['named_entities'], doc['table_context_data'])\n",
    "            print(f\"Abstractive Summary: {abstractive_summary}\\n\")\n",
    "    \n",
    "    elif mode == \"full\":\n",
    "        for doc in all_preprocessed_data:\n",
    "            abstractive_summary = generate_llama_summary_with_context(doc['normalized_text'], doc['named_entities'], doc['table_context_data'])\n",
    "            print(f\"Document: {doc['file_name']}\\nAbstractive Summary: {abstractive_summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9150d60e-3c1f-485c-9d15-6a2106898e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the files\n",
    "pdf_directory = \"path/to/pdf/documents\"\n",
    "all_preprocessed_data = process_files(pdf_directory)\n",
    "\n",
    "# Prepare FAISS index and K-Means clusters\n",
    "num_clusters = 2\n",
    "faiss_index, all_document_embeddings, clusters, kmeans_model = prepare_faiss_and_clusters(all_preprocessed_data, num_clusters)\n",
    "\n",
    "# Run query-based summarization\n",
    "query = \"Patent infringement in intellectual property law\"\n",
    "dynamic_summary_mode(all_preprocessed_data, faiss_index, clusters, kmeans_model, mode=\"query\", query=query, top_k=5)\n",
    "\n",
    "# Run full-document summarization\n",
    "dynamic_summary_mode(all_preprocessed_data, faiss_index, clusters, kmeans_model, mode=\"full\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
