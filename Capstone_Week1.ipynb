{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hpa_YErXpvU",
        "outputId": "65804f30-34e7-4171-d21e-0ecbd8241388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uLoPO12hXweX"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing the text files in Google Drive\n",
        "text_files_dir = \"/content/drive/MyDrive/AIML/Capstone-Project/data/CUAD_v1/full_contract_txt\"  # Replace with your actual folder path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QWkdZaPnU6Ir"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "\n",
        "# Load the spaCy model for NLP tasks\n",
        "nlp = spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RsmaaJYNWSuc"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation (optional, depending on context)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qZhHmh4BWX6b"
      },
      "outputs": [],
      "source": [
        "def remove_noise(text):\n",
        "    text = re.sub(r'EXHIBIT \\d+\\.\\d+', '', text)  # Remove exhibit numbers\n",
        "    text = re.sub(r'Page \\d+', '', text)  # Remove page numbers\n",
        "    text = re.sub(r'CONFIDENTIAL', '', text)  # Remove confidentiality markers\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TvK1UjTuWb7O"
      },
      "outputs": [],
      "source": [
        "def mark_redacted_text(text):\n",
        "    text = re.sub(r'\\[?\\*+\\]?', '[REDACTED]', text)  # Handles cases like \"***\" or \"[***]\"\n",
        "    text = re.sub(r'_+', '[REDACTED]', text)  # Handles cases like \"___\"\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_4LFciKtWe5o"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc]  # Word-level tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9zGsei_qWiRs"
      },
      "outputs": [],
      "source": [
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yewSvnvYWr_B"
      },
      "outputs": [],
      "source": [
        "def handle_special_characters(text):\n",
        "    text = re.sub(r'[^\\w\\s\\$]', '', text)  # Remove all special characters except $\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tFUIdmehWuo7"
      },
      "outputs": [],
      "source": [
        "def segment_text_into_paragraphs(text):\n",
        "    paragraphs = text.split('\\n\\n')  # Simple segmentation based on double newlines\n",
        "    return [para.strip() for para in paragraphs if para.strip()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2ubyFWBKWxmE"
      },
      "outputs": [],
      "source": [
        "def preprocess_document(text):\n",
        "    text = clean_text(text)\n",
        "    text = remove_noise(text)\n",
        "    text = mark_redacted_text(text)\n",
        "    text = handle_special_characters(text)\n",
        "    paragraphs = segment_text_into_paragraphs(text)\n",
        "    named_entities = extract_named_entities(text)\n",
        "    return paragraphs, named_entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gw95w6eAW1rO"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing the text files\n",
        "# text_files_dir = \"/path/to/your/text/files\"  # Replace with your actual directory path\n",
        "\n",
        "# Process all files in the directory\n",
        "all_preprocessed_data = []\n",
        "\n",
        "for file_path in glob(os.path.join(text_files_dir, \"*.txt\")):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        raw_text = file.read()\n",
        "        preprocessed_paragraphs, named_entities = preprocess_document(raw_text)\n",
        "        for para in preprocessed_paragraphs:\n",
        "            all_preprocessed_data.append({\n",
        "                \"file_name\": os.path.basename(file_path),\n",
        "                \"paragraph\": para,\n",
        "                \"named_entities\": named_entities\n",
        "            })\n",
        "\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "df_preprocessed = pd.DataFrame(all_preprocessed_data)\n",
        "\n",
        "# Save the preprocessed data to a CSV file in Google Drive\n",
        "output_path = \"/content/drive/MyDrive/preprocessed_legal_documents.csv\"\n",
        "df_preprocessed.to_csv(output_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fd4NXoH5W-BZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4056e875-f051-47be-a980-0a69c16585d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           file_name  \\\n",
            "0  CytodynInc_20200109_10-Q_EX-10.5_11941634_EX-1...   \n",
            "1  BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGRE...   \n",
            "2  CerenceInc_20191002_8-K_EX-10.4_11827494_EX-10...   \n",
            "3  FTENETWORKS,INC_02_18_2016-EX-99.4-STRATEGIC A...   \n",
            "4  KIROMICBIOPHARMA,INC_05_11_2020-EX-10.23-CONSU...   \n",
            "\n",
            "                                           paragraph  \\\n",
            "0  exhibit 105 certain identified information has...   \n",
            "1  exhibit 993 case 1810248 mfw doc 6321 filed 04...   \n",
            "2  exhibit 104 intellectual property agreement by...   \n",
            "3  exhibit 994 strategic alliance agreement edgef...   \n",
            "4  exhibit 1023 corporate address fannin south pr...   \n",
            "\n",
            "                                      named_entities  \n",
            "0  [(105, CARDINAL), (december 17 2019, DATE), (c...  \n",
            "1  [(993, CARDINAL), (1810248, DATE), (6321, CARD...  \n",
            "2  [(104, CARDINAL), (nuance communications inc, ...  \n",
            "3  [(994, CARDINAL), (this 17t h day of february ...  \n",
            "4  [(1023, DATE), (140, CARDINAL), (7707, CARDINA...  \n"
          ]
        }
      ],
      "source": [
        "# Display a sample of the preprocessed data for manual review\n",
        "print(df_preprocessed.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ropjyj9XnbI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}